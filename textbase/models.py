import os
import re
import openai
from textbase.message import Message
from langchain.agents import AgentType
from langchain.chat_models import ChatOpenAI
from langchain.prompts import MessagesPlaceholder
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory
from langchain import (LLMMathChain,SerpAPIWrapper,SQLDatabase,SQLDatabaseChain,)
from typing import Optional
from sqlmodel import Field, Session, SQLModel, create_engine


# This is the default sql database to be used for storing the data of the user.

# We are storing the id, username, question as well as the response from the model. The id will be generated by default.
class Useritems(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    username: str | None= None
    question: str | None= None
    answer: str | None= None

# Path of the URI where we want to store the data. We are using sqlite database to store the information.
sqlite_url  = "sqlite:///./userdata.db"

# Create the engine
engine = create_engine(sqlite_url, echo=True)

# Creating the database table. 
def create_db_and_tables():
    SQLModel.metadata.create_all(engine)

# A utility function to check if the defualt model is not able to answer properly. If it is not able to answer properly then second model will do its job with more advance features.
def contains_specific_texts(input_string):
    specific_texts = [
        "Sorry, but I don't",
        "I am not trained",
        "I'm sorry, but as an AI",
        "I don't have access to",
        "I apologize",
        "Apologize",
        "I'm sorry",
    ]
    # Using the re library for the pattern matching of the string.
    pattern = "|".join(re.escape(text) for text in specific_texts)
    match = re.search(pattern, input_string)
    
    # Return Boolean value
    return bool(match)


def getResponses(question,max_tokens,username):
    
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    # Use the langchain openai model
    llm = ChatOpenAI(temperature=0.7, model="gpt-4", openai_api_key=OPENAI_API_KEY, max_tokens=max_tokens)

    # Use the google serp api wrapper
    search = SerpAPIWrapper()

    # Use langchain math search chain for mathematical calculations.
    llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)

    # create_db_and_tables()
    db = SQLDatabase.from_uri(sqlite_url)
    
    # Intelligent system with combination of prompt engineering to make it search through the SQL database
    db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)

    # We have three tools available. Searh, calcaulator and the data base. The searh is using the serpapi for google search. Calculator is to perform the mathematical calcualtions. The data base on the other hand searches through the SQL data base.
    tools = [
        Tool(
            name="Search",
            func=search.run,
            description="useful for when you need to answer questions about current events. You should ask targeted questions",
        ),
        Tool(
            name="Calculator",
            func=llm_math_chain.run,
            description="useful for when you need to answer questions about math",
        ),
        Tool(
            name="users",
            func=db_chain.run,
            description="useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context",
        ),
    ]
    # This is the memory maangement response.
    agent_kwargs = {
        "extra_prompt_messages": [MessagesPlaceholder(variable_name="memory")],
    }

    # Use the conversational buffer memory for the memory management.
    memory = ConversationBufferMemory(memory_key="memory", return_messages=True)
    
    # We are initializing the agent with the tools, llm, agent asd well as the memory manager
    agent = initialize_agent(
        tools,
        llm,
        agent=AgentType.OPENAI_FUNCTIONS,
        verbose=True,
        agent_kwargs=agent_kwargs,
        memory=memory,
    )
    # res=agent.run(f"First Search through the database and try to find the context to the question. use only the rows with wher the value of username filed is: '{query.username}. If it is present then give asnwer based on that. Else do anything else."+question)
    print("****************************************************************")
    print(username)

    # Run the agent to get the response. The prompt is very important. 
    res = agent.run(
        f"First try to answer your own without the table or database. If you can't find a good answer then Search through the database and try to find the context to the question. use only the rows where the value of username field is: '{username}'. If that still doesnt work then make a websearch thorugh the serpapi. Do not mention the username in your response. "
        + question
    )

    # Inserting the data after the the model gives some response.
    insert_data = Useritems(username=username,question=question, answer=res)

    with Session(engine) as session:

        # Insert the data. 
        session.add(insert_data)

        # Commit the changes in the database
        session.commit()
    return res


def chatCompletion(model, message_history,system_prompt, temperature, max_tokens,username):

    # Setting the max memory history to be 5 instead to reduce the effect of token limit.
    if len(message_history) > 5:
        message_history = message_history[-5:]
    
    # Getting the response from the chat completion model.
    response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                *map(dict, message_history),
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
    # Getting the response from the model.
    response=response['choices'][0]['message']['content']

    # If the response is not up to the mark then we will use the help of google search or any other agent.
    if(contains_specific_texts(response)==True):
        response = getResponses(message_history[len(message_history) - 1].content, max_tokens,username)
    return response

class OpenAI:
    api_key = None

    @classmethod
    def generate(
        cls,
        system_prompt: str,
        message_history: list[Message],
        model="gpt-3.5-turbo",
        max_tokens=3000,
        temperature=0.7,
        service="chat_completion",
        username="bd01fee8-8725-4158-b21c-f8d8df9c830e"
        
):
        assert cls.api_key is not None, "OpenAI API key is not set"
        openai.api_key = cls.api_key
        
        # Checking whether the length of the message history is zero to handle an error
        if(len(str(message_history[len(message_history) - 1].content))==0):
            return "you did not provide a message"
        
        # If else statement to choose the type of service the user is interested in.
        if(service=='chat_completion'):
            response=chatCompletion(model, message_history,system_prompt, temperature, max_tokens, username)

        elif(service=="gpt_model_chat"):
            response = getResponses(message_history[len(message_history) - 1].content,max_tokens, username)

        # Return the response
        return response
